{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/niloofar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explatory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/niloofar/Downloads/Hotel_Reviews.csv\")\n",
    "\n",
    "# total number of hotels:\n",
    "\n",
    "len(set(df.Hotel_Name))\n",
    "df.Hotel_Name.value_counts()\n",
    "\n",
    "#change the format of time to datetime\n",
    "\n",
    "df['Review_Date']=pd.to_datetime(df['Review_Date'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting one sample hotel with highest number of review\n",
    "df1=df[df.Hotel_Name==\"Britannia International Hotel Canary Wharf\"]\n",
    "\n",
    "df1.index = range(df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x132ec5e80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1['Reviewer_Score'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing for textual variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_text = pd.DataFrame(df1['Positive_Review'])\n",
    "data_text['Neg'] = df1['Negative_Review']\n",
    "data_text['index'] = data_text.index\n",
    "data_text.columns=['pos','neg','index']\n",
    "data_text['pos']=[\"  \" if x == 'No Positive' else x for x in data_text['pos']]\n",
    "data_text['neg']=[\"  \" if x == 'No Negative' else x for x in data_text['neg']]\n",
    "# data_text['review']=data_text['pos']+data_text['neg']\n",
    "# df=data_text['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docp=list(sent_to_words(data_text['pos']))\n",
    "docn=list(sent_to_words(data_text['neg']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_Table=df1[['Review_Date','Reviewer_Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review_Date  Reviewer_Score\n",
       "0  2017-08-03             7.9\n",
       "1  2017-08-03             8.3\n",
       "2  2017-08-02             6.3\n",
       "3  2017-08-02             5.4\n",
       "4  2017-08-02             6.3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Time_Table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_docn=list(data_text['neg'])\n",
    "pre_docp=list(data_text['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_list = [\"Nothing\",\"ok\",\"good\",\"great\",\"excellent\",\"very\",\"london\",\"nice\",\"lovely\",\"okay\",\"like\",\"wharf\",\"canary\",\"room\",\"amaze\",   \n",
    "#              'Indian','Italian','hotel','room','nothing','great','excellent','good','ideal','one','people','pleasant','wa']\n",
    "stop_words=['hotel','room','nothing','would','could','want','go','recommend','everything','be','was','good','ok','great','poor']            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import English, STOP_WORDS\n",
    "# from en_core_web_lg import *\n",
    "# import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(input_data, no_list,time_table,start, limit, step,file_name,verbose):\n",
    "    \n",
    "    #'''data pre processing using spacy '''\n",
    "    nlp = English()\n",
    "    lp= spacy.load(\"en\")\n",
    "    nlp.Defaults.stop_words.update(stop_words)\n",
    "\n",
    "    for word in STOP_WORDS:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        lexeme.is_stop = True\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    def spacy_root(text):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc=[]\n",
    "        l=[]\n",
    "        for word in text:\n",
    "            ss=nlp(word)\n",
    "            for chunk in ss.noun_chunks:\n",
    "                l.append(chunk.root.text)\n",
    "        doc.append(l)\n",
    "        return(doc)\n",
    "\n",
    "\n",
    "    def lemmatizer(doc):\n",
    "        # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "        # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "        doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "        doc = u' '.join(doc)\n",
    "        return nlp.make_doc(doc)\n",
    "\n",
    "    def remove_stopwords(doc):\n",
    "        spacy_nlp = spacy.load('en_core_web_sm')\n",
    "        spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "        for i in no_list:  \n",
    "            STOP_WORDS.add(i)\n",
    "\n",
    "        for word in STOP_WORDS:\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "        tokens = [token.text for token in doc if not token.is_stop and token.is_punct != True and  len(token) >=3]\n",
    "        tokens=[i.lower() for i in tokens]\n",
    "        return tokens\n",
    "    nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "    nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "    nlp.add_pipe(spacy_root,name='root')\n",
    "\n",
    "    def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "            model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=dictionary,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        return model_list, coherence_values\n",
    "    \n",
    "    \n",
    "     #build a topic model\n",
    "    \n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    \n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            \n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "    if verbose==True:\n",
    "            \n",
    "    \n",
    "        doc_list = []\n",
    "        for doc in tqdm(input_data):\n",
    "            pr = nlp(doc)\n",
    "            doc_list.append(pr)\n",
    "            \n",
    "        dd=[flatten(i) for i in doc_list]\n",
    "        \n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(dd, f)\n",
    "        \n",
    "        \n",
    "    else:    \n",
    "        \n",
    "        with open(file_name, 'rb') as f:\n",
    "            dd = pickle.load(f)\n",
    "\n",
    "       \n",
    "    \n",
    "        \n",
    "    words = corpora.Dictionary(dd)\n",
    "    corpus = [words.doc2bow(doc) for doc in dd]\n",
    "    \n",
    "\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=words, corpus=corpus, texts=dd, start=start, limit=limit, step=step)\n",
    "    opt=coherence_values.index(max(coherence_values))\n",
    "    optimal_model=model_list[opt]\n",
    "\n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=input_data)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    df_dominant_topic=df_dominant_topic.merge(time_table, left_index=True, right_index=True)\n",
    "\n",
    "    return(df_dominant_topic)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e6d298efdf40f48abf7b913ee7c772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4789), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "compute_coherence_values() got an unexpected keyword argument 'dictionary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4b4845022ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositive_topic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_docp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTime_Table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d6d2f730981f>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(input_data, no_list, time_table, start, limit, step)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_coherence_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoherence_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0moptimal_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_coherence_values() got an unexpected keyword argument 'dictionary'"
     ]
    }
   ],
   "source": [
    "positive_topic=process(pre_docp, stop_words,Time_Table,start=3, limit=20, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topic=process(pre_docn, stop_words,Time_Table,start=3, limit=20, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_process(table,input_data):\n",
    "    \n",
    "    def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return(\"{}\".format( str(score)))\n",
    "   \n",
    "    aa=[sentiment_analyzer_scores(i) for i in input_data]\n",
    "    sent_score=[ast.literal_eval(i) for i in aa]\n",
    "    SC=pd.DataFrame(sent_score)\n",
    "\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    Table1=table[['Document_No','Review_Date','Reviewer_Score']]\n",
    "    Table2=table[['Document_No','Dominant_Topic']]\n",
    "    Table3=SC[['Document_No','Compound']]\n",
    "    Table4=pd.me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    \n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_docn=list(data_text['neg'])\n",
    "pre_docp=list(data_text['pos'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this function, the dominant topic of each review column for specific date is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "lp= spacy.load(\"en\")\n",
    "nlp.Defaults.stop_words.update(stop_words)\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def spacy_root(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc=[]\n",
    "    l=[]\n",
    "    for word in text:\n",
    "        ss=nlp(word)\n",
    "        for chunk in ss.noun_chunks:\n",
    "            l.append(chunk.root.text)\n",
    "    doc.append(l)\n",
    "    return(doc)\n",
    "\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    \n",
    "    for i in no_list:  \n",
    "        STOP_WORDS.add(i)\n",
    "\n",
    "    for word in STOP_WORDS:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        lexeme.is_stop = True\n",
    "\n",
    "        \n",
    "#     for word in doc:   \n",
    "#           spacy_nlp(word)\n",
    "            \n",
    "    tokens = [token.text for token in doc if not token.is_stop and token.is_punct != True and  len(token) >=3]\n",
    "\n",
    "#     nlp = spacy.load(\"en\")\n",
    "#     doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True and  len(token) >=3]\n",
    "    tokens=[i.lower() for i in tokens]\n",
    "    return tokens\n",
    "\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "nlp.add_pipe(spacy_root,name='root')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_list = []\n",
    "no_list=stop_words\n",
    "\n",
    "for doc in tqdm(pre_docp):\n",
    "    pr = nlp(doc)\n",
    "    doc_list.append(pr)\n",
    "dd=[flatten(i) for i in doc_list]\n",
    "words = corpora.Dictionary(dd)\n",
    "corpus = [words.doc2bow(doc) for doc in dd]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "\n",
    "# Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_topic=format_topics_sentences(lda_model, corpus, pre_docp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_listn = []\n",
    "no_list=stop_words\n",
    "\n",
    "for doc in tqdm(pre_docn):\n",
    "    pr = nlp(doc)\n",
    "    doc_listn.append(pr)\n",
    "ddn=[flatten(i) for i in doc_listn]\n",
    "words = corpora.Dictionary(ddn)\n",
    "corpus = [words.doc2bow(doc) for doc in ddn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NegTopic=format_topics_sentences(lda_model, corpus, pre_docn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NegTopic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(NegTopic['Topic_Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def compute_coherence_values(id2word, corpus, texts, limit=10, start=2, step=1):\n",
    "\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "            model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        return model_list, coherence_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(ddn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_coherence_values(words, corpus, ddn, limit=15, start=3, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherencemodel.get_coherence()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(doc,pre_doc,time_table,stop_list):\n",
    "    \n",
    "    nlp = English()\n",
    "    lp= spacy.load(\"en\")\n",
    "    nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "    for word in STOP_WORDS:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        lexeme.is_stop = True\n",
    "           \n",
    "            \n",
    "    \n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    def spacy_root(text):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc=[]\n",
    "        l=[]\n",
    "        for word in text:\n",
    "            ss=nlp(word)\n",
    "            for chunk in ss.noun_chunks:\n",
    "                l.append(chunk.root.text)\n",
    "        doc.append(l)\n",
    "        return(doc)\n",
    "            \n",
    "\n",
    "    def lemmatizer(doc):\n",
    "        # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "        # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "        doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "        doc = u' '.join(doc)\n",
    "        return nlp.make_doc(doc)\n",
    "\n",
    "    def remove_stopwords(doc):\n",
    "        # This will remove stopwords and punctuation.\n",
    "        # Use token.text to return strings, which we'll need for Gensim.\n",
    "        nlp = spacy.load(\"en\")\n",
    "        doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True and  len(token) >=3]\n",
    "        doc=[i.lower() for i in doc]\n",
    "        return doc\n",
    "\n",
    "    nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "    nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "    nlp.add_pipe(spacy_root,name='root')\n",
    "    \n",
    "    doc_list = []\n",
    "    for doc in tqdm(pre_docp):\n",
    "        pr = nlp(doc)\n",
    "        doc_list.append(pr)\n",
    "    dd=[flatten(i) for i in doc_list]\n",
    "    words = corpora.Dictionary(dd)\n",
    "    corpus = [words.doc2bow(doc) for doc in dd]\n",
    "\n",
    "    def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "            model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        return model_list, coherence_values\n",
    "    \n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    \n",
    "    # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "    doc_list = []\n",
    "    for doc in tqdm(pre_docp):\n",
    "        pr = nlp(doc)\n",
    "        doc_list.append(pr)\n",
    "\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=20, step=1)\n",
    "    opt=coherence_values.index(max(coherence_values))\n",
    "    optimal_model=model_list[opt]\n",
    "\n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=pre_doc)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    df_dominant_topic=df_dominant_topic.merge(time_table, left_index=True, right_index=True)\n",
    "\n",
    "    return(df_dominant_topic)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_topics=process(docp,pre_docp,df1['Review_Date'],stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topics=process(docn,pre_docn,df1['Review_Date'],stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_topics['Keywords'][105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_topics['Text'][105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topics['Keywords'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topics['Text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(negative_topics['Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_topics=process(docp,pre_docp,df1['Review_Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Each Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit=20, start=3, step=2):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "\n",
    "# Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "#     print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "    return(\"{}\".format( str(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=[sentiment_analyzer_scores(i) for i in pre_docn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "sent_score=[ast.literal_eval(i) for i in aa]\n",
    "SC=pd.DataFrame(sent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import English, STOP_WORDS\n",
    "# from en_core_web_lg import *\n",
    "# import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "lp= spacy.load(\"en\")\n",
    "#     stop_list=[i.lower() for i in stop_list]\n",
    "\n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_bigrams(doc):\n",
    "#         return [bigram_mod[t] for t in doc]\n",
    "\n",
    "# def make_trigrams(doc):\n",
    "#     return [trigram_mod[bigram_mod[t]] for t in doc]\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def spacy_root(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc=[]\n",
    "    l=[]\n",
    "    for word in text:\n",
    "        ss=nlp(word)\n",
    "        for chunk in ss.noun_chunks:\n",
    "            l.append(chunk.root.text)\n",
    "    doc.append(l)\n",
    "    return(doc)\n",
    "            \n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    nlp = spacy.load(\"en\")\n",
    "    nlp.Defaults.stop_words |= {\"Nothing\",\"ok\",\"good\",\"great\",\"excellent\",\"very\",\"london\",\"nice\",\"lovely\",\"okay\",\"like\",\"wharf\",\"canary\",\"room\",\"amaze\",   \n",
    "             'Indian','Italian','hotel','room','nothing','great','excellent','good','ideal','one','people','pleasant','wa'}\n",
    "\n",
    "             \n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True and  len(token) >3]\n",
    "    doc=[i.lower() for i in doc]\n",
    "    return doc\n",
    "\n",
    "\n",
    "                 \n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "# nlp.add_pipe(low,name='lower_punctuation')\n",
    "\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "# nlp.add_pipe(spacy_root,name='root')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in tqdm(pre_docp):\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(doc)\n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=[flatten(i) for i in doc_list]\n",
    "words = corpora.Dictionary(dd)\n",
    "\n",
    "corpus = [words.doc2bow(doc) for doc in dd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_wordnet_pos(word):    \n",
    "    \n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def word_lemmatizer(text):   \n",
    "    '''lemamtizes the tokens based on their part of speech''' \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "#      doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    return text\n",
    "def reflection_tokenizer(text):   \n",
    "    '''expects a string an returns a list of lemmatized tokens \n",
    "        and removes the stop words. Tokens are lower cased and \n",
    "        non- alphanumeric characters as well as numbers removed. '''   \n",
    "    text=re.sub(r'[\\W_]+', ' ', text) #keeps alphanumeric characters\n",
    "    text=re.sub(r'\\d+', '', text) #removes numbers\n",
    "    text = text.lower()\n",
    "    tokens = [word for word in word_tokenize(text)]\n",
    "    tokens = [word for word in tokens if len(word) >=3]\n",
    "    #removes smaller than 3 character\n",
    "    tokens = [word_lemmatizer(w) for w in tokens]\n",
    "    additional_stop_words=['hotel','room','nothing','great','excellent','good','ideal','one','people','pleasant','wa']\n",
    "    tokens = [s for s in tokens if s not in stop_words and s not in additional_stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=pre_docn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectorizer = CountVectorizer(tokenizer = reflection_tokenizer, min_df=10, stop_words=stop_words, ngram_range=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = token_vectorizer.fit_transform(data_text['pos'])\n",
    "# X = token_vectorizer.fit_transform(pre_docp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidedlda\n",
    "tf_feature_names = token_vectorizer.get_feature_names()\n",
    "word2id = dict((v, idx) for idx, v in enumerate(tf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list=[['staff','receptionist','service','reception','cleaner'],\n",
    "                  ['location','proximity','place','parking','view','window','distance','pizzeria'],\n",
    "                 ['clean','cleanliness','bed','bathroom','towel','toilet'],\n",
    "                 ['carpet','balcony','interior','staircase','light','decoration','furniture','inside'],\n",
    "                 ['breakfast','bar', 'restaurant','food','pizza','coffee','eat'],\n",
    "                 ['value','price','money','affordable','business','free','cost','cheap','expensive'],\n",
    "                 ['spa', 'jacuzzi','gym','swimming']\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = guidedlda.GuidedLDA(n_topics=8, n_iter=100, random_state=7, refresh=10)\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)\n",
    "# model.fit(dd, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 15\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "         topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "         print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(pre_docn)\n",
    "X = cv.transform(pre_docn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=10, stop_words=stop_words, ngram_range=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectorizer = CountVectorizer(tokenizer = reflection_tokenizer, min_df=10, stop_words=stop_words, ngram_range=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list={'staff':0,'location':1,'clean':2,'neutral':3,'breakfast':4,'bar':4,'value':5,'price':5,'money':5,'spa':6,'jacuzzi':6,'food':4,\n",
    " 'restaurant':4,'friendly':0,'wharf':1,'canary':1,'cleanliness':2,'room':7,'bed':7,'bathroom':7,'balcony':7, 'gym':6,'swimming_pool':6,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidedlda\n",
    "\n",
    "model = guidedlda.GuidedLDA(n_topics=7, n_iter=100, random_state=7, refresh=10)\n",
    "seed_topics = {}\n",
    "# for t_id, st in enumerate(seed_topic_list):\n",
    "#     for word in st:\n",
    "#         seed_topics[word2id[word]] = t_id\n",
    "model.fit(X, seed_topics=seed_topic_list, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 15\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(docn)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_docn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(doc, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[doc], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trigram[trigram_mod[doc[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmitize and stemming option (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(doc)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(lda_model.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(lda_model.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "ldamodel=optimal_model\n",
    "    # Get main topic in each document\n",
    "for i, row_list in enumerate(ldamodel[corpus]):\n",
    "    row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "    # print(row)\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "\n",
    "# Get main topic in each document\n",
    "for i, row_list in enumerate(ldamodel[corpus]):\n",
    "    row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "    # print(row)\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "    # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=pd.DataFrame(row)\n",
    "row.columns=['topic_num', 'prop_topic']\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[4]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=5))\n",
    "\n",
    "# What is the Dominant topic and its percentage contribution in each document\n",
    "\n",
    "def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=doc):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "           row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "          # print(row)\n",
    "           row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "          # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "           for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=list(data_text['review']))\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:10]\n",
    "\n",
    "set(df_dominant_topic['Dominant_Topic'])\n",
    "\n",
    "model_topics=model_topics[1:4]\n",
    "\n",
    "df_dominant_topic['Text'][7]\n",
    "\n",
    "df_dominant_topic['Keywords'][7]\n",
    "\n",
    "df_topic_sents_keywords['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=20, step=1)\n",
    "\n",
    "\n",
    "# Show graph\n",
    "limit=20; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
