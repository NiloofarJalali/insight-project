{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/niloofar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explatory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/niloofar/Downloads/Hotel_Reviews.csv\")\n",
    "\n",
    "# total number of hotels:\n",
    "\n",
    "len(set(df.Hotel_Name))\n",
    "df.Hotel_Name.value_counts()\n",
    "\n",
    "#change the format of time to datetime\n",
    "\n",
    "df['Review_Date']=pd.to_datetime(df['Review_Date'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting one sample hotel with highest number of review\n",
    "df1=df[df.Hotel_Name==\"Britannia International Hotel Canary Wharf\"]\n",
    "\n",
    "df1.index = range(df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x132ec5e80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1['Reviewer_Score'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing for textual variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_text = pd.DataFrame(df1['Positive_Review'])\n",
    "data_text['Neg'] = df1['Negative_Review']\n",
    "data_text['index'] = data_text.index\n",
    "data_text.columns=['pos','neg','index']\n",
    "data_text['pos']=[\"  \" if x == 'No Positive' else x for x in data_text['pos']]\n",
    "data_text['neg']=[\"  \" if x == 'No Negative' else x for x in data_text['neg']]\n",
    "# data_text['review']=data_text['pos']+data_text['neg']\n",
    "# df=data_text['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docp=list(sent_to_words(data_text['pos']))\n",
    "docn=list(sent_to_words(data_text['neg']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_Table=df1[['Review_Date','Reviewer_Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review_Date  Reviewer_Score\n",
       "0  2017-08-03             7.9\n",
       "1  2017-08-03             8.3\n",
       "2  2017-08-02             6.3\n",
       "3  2017-08-02             5.4\n",
       "4  2017-08-02             6.3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Time_Table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_docn=list(data_text['neg'])\n",
    "pre_docp=list(data_text['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_list = [\"Nothing\",\"ok\",\"good\",\"great\",\"excellent\",\"very\",\"london\",\"nice\",\"lovely\",\"okay\",\"like\",\"wharf\",\"canary\",\"room\",\"amaze\",   \n",
    "#              'Indian','Italian','hotel','room','nothing','great','excellent','good','ideal','one','people','pleasant','wa']\n",
    "stop_words=['hotel','room','nothing','would','could','want','go','recommend','everything','be','was','good','ok','great','poor']            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import English, STOP_WORDS\n",
    "# from en_core_web_lg import *\n",
    "# import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(input_data, no_list,time_table,start, limit, step,file_name,verbose):\n",
    "    \n",
    "    #'''data pre processing using spacy '''\n",
    "    nlp = English()\n",
    "    lp= spacy.load(\"en\")\n",
    "    nlp.Defaults.stop_words.update(stop_words)\n",
    "\n",
    "    for word in STOP_WORDS:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        lexeme.is_stop = True\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    def spacy_root(text):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc=[]\n",
    "        l=[]\n",
    "        for word in text:\n",
    "            ss=nlp(word)\n",
    "            for chunk in ss.noun_chunks:\n",
    "                l.append(chunk.root.text)\n",
    "        doc.append(l)\n",
    "        return(doc)\n",
    "\n",
    "\n",
    "    def lemmatizer(doc):\n",
    "        # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "        # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "        doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "        doc = u' '.join(doc)\n",
    "        return nlp.make_doc(doc)\n",
    "\n",
    "    def remove_stopwords(doc):\n",
    "        spacy_nlp = spacy.load('en_core_web_sm')\n",
    "        spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "        for i in no_list:  \n",
    "            STOP_WORDS.add(i)\n",
    "\n",
    "        for word in STOP_WORDS:\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "        tokens = [token.text for token in doc if not token.is_stop and token.is_punct != True and  len(token) >=3]\n",
    "        tokens=[i.lower() for i in tokens]\n",
    "        return tokens\n",
    "    nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "    nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "    nlp.add_pipe(spacy_root,name='root')\n",
    "\n",
    "    def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "            model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=dictionary,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        return model_list, coherence_values\n",
    "    \n",
    "    \n",
    "     #build a topic model\n",
    "    \n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    \n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            \n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "    if verbose==True:\n",
    "            \n",
    "    \n",
    "        doc_list = []\n",
    "        for doc in tqdm(input_data):\n",
    "            pr = nlp(doc)\n",
    "            doc_list.append(pr)\n",
    "            \n",
    "        dd=[flatten(i) for i in doc_list]\n",
    "        \n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(dd, f)\n",
    "        \n",
    "        \n",
    "    else:    \n",
    "        \n",
    "        with open(file_name, 'rb') as f:\n",
    "            dd = pickle.load(f)\n",
    "\n",
    "       \n",
    "    \n",
    "        \n",
    "    words = corpora.Dictionary(dd)\n",
    "    corpus = [words.doc2bow(doc) for doc in dd]\n",
    "    \n",
    "\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=words, corpus=corpus, texts=dd, start=start, limit=limit, step=step)\n",
    "    opt=coherence_values.index(max(coherence_values))\n",
    "    optimal_model=model_list[opt]\n",
    "\n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=input_data)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    df_dominant_topic=df_dominant_topic.merge(time_table, left_index=True, right_index=True)\n",
    "\n",
    "    return(df_dominant_topic)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a980aeead503428aa3ad4b8465b72deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4789), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "positive_topic=process(input_data=pre_docp, no_list=stop_words,time_table=Time_Table,start=3, limit=20, step=1,file_name=\"doc_pos\",verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_topic=process(input_data=pre_docp, no_list=stop_words,time_table=Time_Table,start=2, limit=10, step=1,file_name=\"doc_pos\",verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/niloofar/Documents/insight/data/cleaned/hotel1/positive\", 'wb') as f:\n",
    "            pickle.dump(positive_topic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d95dce00f6340be9a72edcc6875d352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4789), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative_topic=process(input_data=pre_docn, no_list=stop_words,time_table=Time_Table,start=3, limit=20, step=1,file_name=\"doc_neg\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/niloofar/Documents/insight/data/cleaned/hotel1/negative\", 'wb') as f:\n",
    "            pickle.dump(negative_topic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_pos=pd.DataFrame({\"Dominant_Topic\": [0,1,2,3,4,5,6,7,8],\n",
    "                    \"Topic_Name\":[\"Decoration\",\"facility\",\"location\",\n",
    "                                  \"price\",\"amenities\",\"parking\",\"general\",\"staff_receptionist\",\"Room_size\"]})\n",
    "                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_neg=pd.DataFrame({\"Dominant_Topic\": [0,1,2,3,4,5,6,7],\n",
    "                    \"Topic_Name\":[\"general\",\"service_food\",\"air_conditioning\",\n",
    "                                  \"staff_receptionist\",\"Decoration\",\"bed/bath/shower\",\"wifi\",\"parking\"]})\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_modify (table,topic_table):\n",
    "    table=table.merge(topic_table, on=['Dominant_Topic'])\n",
    "    table.loc[table.Topic_Perc_Contrib<0.3 ,'Topic_Name']='None'\n",
    "    return(table[['Document_No','Review_Date','Reviewer_Score','Topic_Name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_topic_final=table_modify(positive_topic,topic_pos)\n",
    "neg_topic_final=table_modify(negative_topic,topic_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>Topic_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>7.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.4</td>\n",
       "      <td>bed/bath/shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.4</td>\n",
       "      <td>bed/bath/shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>7.5</td>\n",
       "      <td>bed/bath/shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>8.8</td>\n",
       "      <td>bed/bath/shower</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No Review_Date  Reviewer_Score       Topic_Name\n",
       "0            0  2017-08-03             7.9             None\n",
       "1            3  2017-08-02             5.4  bed/bath/shower\n",
       "2            7  2017-08-02             5.4  bed/bath/shower\n",
       "3            9  2017-08-02             7.5  bed/bath/shower\n",
       "4           10  2017-08-02             8.8  bed/bath/shower"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_topic_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_process(table,input_data):\n",
    "    \n",
    "    \n",
    "    def sentiment_analyzer_scores(sentence):\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        return(\"{}\".format(str(score)))\n",
    "   \n",
    "    aa=[sentiment_analyzer_scores(i) for i in input_data]\n",
    "    sent_score=[ast.literal_eval(i) for i in aa]\n",
    "    SC=pd.DataFrame(sent_score)\n",
    "    SC['Document_No']=SC.index\n",
    "    SC=SC[['compound','Document_No']]\n",
    "    table2=table.merge(SC, on=['Document_No'])\n",
    "    table2.loc[table2.Topic_Name==\"None\" ,'compound']=0\n",
    "    return(table2)\n",
    "    \n",
    "#     return(table2.pivot(index='Document_No',columns='Topic_Name',values='neg'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentiment=table_process(pos_topic_final,pre_docp)\n",
    "neg_sentiment=table_process(neg_topic_final,pre_docn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp=pos_sentiment[['Document_No','Topic_Name','compound']]\n",
    "Tn=neg_sentiment[['Document_No','Topic_Name','compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp1=Tp.pivot(index='Document_No',columns='Topic_Name',values='compound')\n",
    "Tn1=Tn.pivot(index='Document_No',columns='Topic_Name',values='compound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp1=Tp1.drop(['None'],axis=1)\n",
    "Tn1=Tn1.drop(['None'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4789"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tp1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns=list(set(Tp1.columns).intersection(Tn1.columns))\n",
    "common_columns.append('Document_No')\n",
    "# common_columns.remove('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp1=Tn1.merge(Tp1,on=common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tp1=Tp1.drop(['None'],axis=1)\n",
    "Tp1=Tp1.fillna(0)\n",
    "Tp1=Tp1.merge(neg_topic_final[['Document_No','Review_Date','Reviewer_Score']],on=['Document_No'])\n",
    "# Tp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_col=['general','Decoration','parking','staff_receptionist']\n",
    "Tp1=Tp1.drop(zero_col,axis=1)\n",
    "Tp1['sum']=Tp1['air_conditioning']+Tp1['bed/bath/shower']+Tp1['service_food']+Tp1['wifi']+Tp1['Room_size']+Tp1['amenities']+Tp1['facility']+Tp1['location']+Tp1['price']\n",
    "Tp1=Tp1[Tp1['sum']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>air_conditioning</th>\n",
       "      <th>bed/bath/shower</th>\n",
       "      <th>service_food</th>\n",
       "      <th>wifi</th>\n",
       "      <th>Room_size</th>\n",
       "      <th>amenities</th>\n",
       "      <th>facility</th>\n",
       "      <th>location</th>\n",
       "      <th>price</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.4</td>\n",
       "      <td>-0.5112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8843</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.3782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.5263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.1123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  air_conditioning  bed/bath/shower  service_food    wifi  \\\n",
       "0            0               0.0           0.0000           0.0  0.0000   \n",
       "3            3               0.0          -0.9578           0.0  0.0000   \n",
       "4            4               0.0           0.0000           0.0  0.4939   \n",
       "6            7               0.0          -0.2006           0.0  0.0000   \n",
       "7            9               0.0           0.2263           0.0  0.0000   \n",
       "\n",
       "   Room_size  amenities  facility  location   price Review_Date  \\\n",
       "0        0.0        0.0       0.0       0.0  0.5719  2017-08-03   \n",
       "3        0.0        0.0       0.0       0.0  0.4466  2017-08-02   \n",
       "4        0.0        0.0       0.0       0.0  0.8843  2017-08-02   \n",
       "6        0.0        0.0       0.0       0.0  0.7269  2017-08-02   \n",
       "7        0.0        0.0       0.0       0.0  0.8860  2017-08-02   \n",
       "\n",
       "   Reviewer_Score     sum  \n",
       "0             7.9  0.5719  \n",
       "3             5.4 -0.5112  \n",
       "4             6.3  1.3782  \n",
       "6             5.4  0.5263  \n",
       "7             7.5  1.1123  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xx=Tp1[['Decoration','Review_Date']]\n",
    "\n",
    "Tp1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "predic_fn_rf=lambda x: model.rf.predict_proba(x).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Staff Those of you with poker faces you should smile more you should ask a guest whether is in a need of help with luggage not just whether he she wants to pay in advance for breakfast or open an account with you or whether would prefer a more expensive room Also saying good morning and good evening to your guests once in a while is good manners Looking like a gentleman won t make you one after all it s not everything in life about money Wifi Rooms with windows at an extra charge of 20 pounds day Taxi companies you co operate with that instead of 5 mins they turn up in 35 It s not your fault entirely but most definitely not your guests '"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_topic.loc[25,'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>location, staff, value, breakfast, money, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>service, book, time, business, fault, bite, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.0</td>\n",
       "      <td>size, night, space, cost, minute, day, bath, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>view, office, window, river, date, rooms, bedr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>wharf, food, place, lot, deal, plenty, meeting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>price, bed, position, arrival, house, ease, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.0</td>\n",
       "      <td>facility, restaurant, bathroom, floor, cleanli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.0</td>\n",
       "      <td>bar, area, tube, station, park, distance, atmo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4.0</td>\n",
       "      <td>reception, style, furniture, stuff, coffee, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dominant_Topic                                           Keywords\n",
       "0              3.0  location, staff, value, breakfast, money, love...\n",
       "5              7.0  service, book, time, business, fault, bite, co...\n",
       "6              8.0  size, night, space, cost, minute, day, bath, f...\n",
       "11             2.0  view, office, window, river, date, rooms, bedr...\n",
       "14             6.0  wharf, food, place, lot, deal, plenty, meeting...\n",
       "26             1.0  price, bed, position, arrival, house, ease, re...\n",
       "62             0.0  facility, restaurant, bathroom, floor, cleanli...\n",
       "69             5.0  bar, area, tube, station, park, distance, atmo...\n",
       "94             4.0  reception, style, furniture, stuff, coffee, re..."
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_topic[['Dominant_Topic','Keywords']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>Topic_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>7.9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.4</td>\n",
       "      <td>bed/bath/shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.4</td>\n",
       "      <td>bed/bath/shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>7.5</td>\n",
       "      <td>bed/bath/shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>8.8</td>\n",
       "      <td>bed/bath/shower</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No Review_Date  Reviewer_Score       Topic_Name\n",
       "0            0  2017-08-03             7.9             None\n",
       "1            3  2017-08-02             5.4  bed/bath/shower\n",
       "2            7  2017-08-02             5.4  bed/bath/shower\n",
       "3            9  2017-08-02             7.5  bed/bath/shower\n",
       "4           10  2017-08-02             8.8  bed/bath/shower"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_topic_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=[sentiment_analyzer_scores(i) for i in pre_docn]\n",
    "aa\n",
    "sent_score=[ast.literal_eval(i) for i in aa]\n",
    "SC=pd.DataFrame(sent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1=neg_topic_final[['Document_No','Topic_Name']]\n",
    "table2=table1.merge(SC, on=['Document_No'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document_No                                                        2579\n",
       "Dominant_Topic                                                        0\n",
       "Topic_Perc_Contrib                                               0.2763\n",
       "Keywords              wasn, rooms, train, fact, age, bag, face, heat...\n",
       "Text                   Room was very noisy from clanking of steam pipes\n",
       "Review_Date                                         2017-03-24 00:00:00\n",
       "Reviewer_Score                                                      6.7\n",
       "Topic_Name                                                         None\n",
       "Name: 4784, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_topic.loc[4784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
