{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/niloofar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explatory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/niloofar/Downloads/Hotel_Reviews.csv\")\n",
    "\n",
    "# total number of hotels:\n",
    "\n",
    "len(set(df.Hotel_Name))\n",
    "df.Hotel_Name.value_counts()\n",
    "\n",
    "#change the format of time to datetime\n",
    "\n",
    "df['Review_Date']=pd.to_datetime(df['Review_Date'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting one sample hotel with highest number of review\n",
    "df1=df[df.Hotel_Name==\"Britannia International Hotel Canary Wharf\"]\n",
    "\n",
    "df1.index = range(df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10d68bd68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1['Reviewer_Score'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing for textual variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_text = pd.DataFrame(df1['Positive_Review'])\n",
    "data_text['Neg'] = df1['Negative_Review']\n",
    "data_text['index'] = data_text.index\n",
    "data_text.columns=['pos','neg','index']\n",
    "data_text['pos']=[\"  \" if x == 'No Positive' else x for x in data_text['pos']]\n",
    "data_text['neg']=[\"  \" if x == 'No Negative' else x for x in data_text['neg']]\n",
    "# data_text['review']=data_text['pos']+data_text['neg']\n",
    "# df=data_text['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docp=list(sent_to_words(data_text['pos']))\n",
    "docn=list(sent_to_words(data_text['neg']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-13-6188e1a484f6>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-6188e1a484f6>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for doc in tqdm(input_data):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(dd, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " 'real',\n",
       " 'complaints',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'was',\n",
       " 'great',\n",
       " 'great',\n",
       " 'location',\n",
       " 'surroundings',\n",
       " 'rooms',\n",
       " 'amenities',\n",
       " 'and',\n",
       " 'service',\n",
       " 'two',\n",
       " 'recommendations',\n",
       " 'however',\n",
       " 'firstly',\n",
       " 'the',\n",
       " 'staff',\n",
       " 'upon',\n",
       " 'check',\n",
       " 'in',\n",
       " 'are',\n",
       " 'very',\n",
       " 'confusing',\n",
       " 'regarding',\n",
       " 'deposit',\n",
       " 'payments',\n",
       " 'and',\n",
       " 'the',\n",
       " 'staff',\n",
       " 'offer',\n",
       " 'you',\n",
       " 'upon',\n",
       " 'checkout',\n",
       " 'to',\n",
       " 'refund',\n",
       " 'your',\n",
       " 'original',\n",
       " 'payment',\n",
       " 'and',\n",
       " 'you',\n",
       " 'can',\n",
       " 'make',\n",
       " 'new',\n",
       " 'one',\n",
       " 'bit',\n",
       " 'confusing',\n",
       " 'secondly',\n",
       " 'the',\n",
       " 'on',\n",
       " 'site',\n",
       " 'restaurant',\n",
       " 'is',\n",
       " 'bit',\n",
       " 'lacking',\n",
       " 'very',\n",
       " 'well',\n",
       " 'thought',\n",
       " 'out',\n",
       " 'and',\n",
       " 'excellent',\n",
       " 'quality',\n",
       " 'food',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'of',\n",
       " 'vegetarian',\n",
       " 'or',\n",
       " 'vegan',\n",
       " 'background',\n",
       " 'but',\n",
       " 'even',\n",
       " 'wrap',\n",
       " 'or',\n",
       " 'toasted',\n",
       " 'sandwich',\n",
       " 'option',\n",
       " 'would',\n",
       " 'be',\n",
       " 'great',\n",
       " 'aside',\n",
       " 'from',\n",
       " 'those',\n",
       " 'minor',\n",
       " 'minor',\n",
       " 'things',\n",
       " 'fantastic',\n",
       " 'spot',\n",
       " 'and',\n",
       " 'will',\n",
       " 'be',\n",
       " 'back',\n",
       " 'when',\n",
       " 'return',\n",
       " 'to',\n",
       " 'amsterdam']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = list ( docp )\n",
    "\n",
    "input_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516b9bdd097d4bdab0a577bbee1ab7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=515738), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3bfd73804f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdoc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdoc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             )\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "input_data = list ( docp )\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "\n",
    "doc_list = []\n",
    "for doc in tqdm(input_data):\n",
    "    pr = nlp(doc)\n",
    "    doc_list.append(pr)\n",
    "\n",
    "dd=[flatten(i) for i in doc_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_Table=df1[['Review_Date','Reviewer_Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_Table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_docn=list(data_text['neg'])\n",
    "pre_docp=list(data_text['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_list = [\"Nothing\",\"ok\",\"good\",\"great\",\"excellent\",\"very\",\"london\",\"nice\",\"lovely\",\"okay\",\"like\",\"wharf\",\"canary\",\"room\",\"amaze\",   \n",
    "#              'Indian','Italian','hotel','room','nothing','great','excellent','good','ideal','one','people','pleasant','wa']\n",
    "stop_words=['hotel','room','nothing','would','could','want','go','recommend','everything','be','was','good','ok','great','poor','miami','YVE']            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import English, STOP_WORDS\n",
    "# from en_core_web_lg import *\n",
    "# import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(input_data, no_list,time_table,start, limit, step,file_name,verbose):\n",
    "    \n",
    "    #'''data pre processing using spacy '''\n",
    "    nlp = English()\n",
    "    lp= spacy.load(\"en\")\n",
    "    nlp.Defaults.stop_words.update(stop_words)\n",
    "\n",
    "    for word in STOP_WORDS:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        lexeme.is_stop = True\n",
    "\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    def spacy_root(text):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc=[]\n",
    "        l=[]\n",
    "        for word in text:\n",
    "            ss=nlp(word)\n",
    "            for chunk in ss.noun_chunks:\n",
    "                l.append(chunk.root.text)\n",
    "        doc.append(l)\n",
    "        return(doc)\n",
    "\n",
    "\n",
    "    def lemmatizer(doc):\n",
    "        # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "        # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "        doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "        doc = u' '.join(doc)\n",
    "        return nlp.make_doc(doc)\n",
    "\n",
    "    def remove_stopwords(doc):\n",
    "        spacy_nlp = spacy.load('en_core_web_sm')\n",
    "        spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "        for i in no_list:  \n",
    "            STOP_WORDS.add(i)\n",
    "\n",
    "        for word in STOP_WORDS:\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "        tokens = [token.text for token in doc if not token.is_stop and token.is_punct != True and  len(token) >=3]\n",
    "        tokens=[i.lower() for i in tokens]\n",
    "        return tokens\n",
    "    nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "    nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "    nlp.add_pipe(spacy_root,name='root')\n",
    "\n",
    "    def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "            model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=dictionary,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        return model_list, coherence_values\n",
    "    \n",
    "    \n",
    "     #build a topic model\n",
    "    \n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    \n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            \n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "    if verbose==True:\n",
    "            \n",
    "    \n",
    "        doc_list = []\n",
    "        for doc in tqdm(input_data):\n",
    "            pr = nlp(doc)\n",
    "            doc_list.append(pr)\n",
    "            \n",
    "        dd=[flatten(i) for i in doc_list]\n",
    "        \n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(dd, f)\n",
    "        \n",
    "        \n",
    "    else:    \n",
    "        \n",
    "        with open(file_name, 'rb') as f:\n",
    "            dd = pickle.load(f)\n",
    "\n",
    "       \n",
    "    \n",
    "        \n",
    "    words = corpora.Dictionary(dd)\n",
    "    corpus = [words.doc2bow(doc) for doc in dd]\n",
    "    \n",
    "\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=words, corpus=corpus, texts=dd, start=start, limit=limit, step=step)\n",
    "    opt=coherence_values.index(max(coherence_values))\n",
    "    optimal_model=model_list[opt]\n",
    "\n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=input_data)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    df_dominant_topic=df_dominant_topic.merge(time_table, left_index=True, right_index=True)\n",
    "\n",
    "    return(df_dominant_topic)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "\n",
    "        with open ( path, 'rb' ) as f:\n",
    "            trip_ad = pickle.load ( f )\n",
    "            Time_Table = trip_ad[ [ 'Date' , 'rating' ] ]\n",
    "            # file_name = '/Users/niloofar/Documents/insight/data/cleaned/hotel2/doc_pos'\n",
    "            input_data = list ( trip_ad[ 'Review' ] )\n",
    "        return  input_data, Time_Table\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f9b5e9980c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Users/niloofar/Documents/insight/data/cleaned/hotel2/doc_pos\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mTopic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTime_Table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'process' is not defined"
     ]
    }
   ],
   "source": [
    "input_data, Time_Table=load_data(\"/Users/niloofar/Documents/insight/HotelWatch/data/trip_ad\")\n",
    "file_name=\"/Users/niloofar/Documents/insight/data/cleaned/hotel2/doc_pos\"\n",
    "\n",
    "Topic=process(input_data=input_data, no_list=stop_words,time_table=Time_Table,start=3, limit=15, step=1,file_name=file_name,verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location, restaurant, view, staff, downtown, miami, street, shop, breakfast, price',\n",
       " 'miami, service, place, desk, staff, experience, city, trip, love, time',\n",
       " 'night, cruise, bathroom, bed, day, elevator, time, book, floor, port',\n",
       " 'park, car, arena, block, towel, space, airlines, taxi, game, concert',\n",
       " 'water, coffee, deal, charge, pool, plan, year, fee, order, request'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(Topic.Keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/Users/niloofar/Documents/insight/data/cleaned/hotel1/positive\", 'wb') as f:\n",
    "#             pickle.dump(positive_topic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topic=process(input_data=pre_docn, no_list=stop_words,time_table=Time_Table,start=3, limit=20, step=1,file_name=\"doc_neg\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/Users/niloofar/Documents/insight/data/cleaned/hotel1/negative\", 'wb') as f:\n",
    "#             pickle.dump(negative_topic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_pos=pd.DataFrame({\"Dominant_Topic\": [0,1,2,3,4,5,6,7,8],\n",
    "                    \"Topic_Name\":[\"Decoration\",\"facility\",\"location\",\n",
    "                                  \"price\",\"amenities\",\"parking\",\"general\",\"staff_receptionist\",\"Room_size\"]})\n",
    "                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_neg=pd.DataFrame({\"Dominant_Topic\": [0,1,2,3,4,5,6,7],\n",
    "                    \"Topic_Name\":[\"general\",\"service_food\",\"air_conditioning\",\n",
    "                                  \"staff_receptionist\",\"Decoration\",\"bed/bath/shower\",\"wifi\",\"parking\"]})\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_modify (table,topic_table):\n",
    "    table=table.merge(topic_table, on=['Dominant_Topic'])\n",
    "#     table.loc[table.Topic_Perc_Contrib<0.3 ,'Topic_Name']='None'\n",
    "    return(table[['Document_No','Review_Date','Reviewer_Score','Topic_Name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/niloofar/Documents/insight/data/cleaned/hotel1/negative\",'rb') as f:\n",
    "        negative=pickle.load(f,encoding='latin1')\n",
    "        \n",
    "with open(\"/Users/niloofar/Documents/insight/data/cleaned/hotel1/positive\",'rb') as f:\n",
    "        positive=pickle.load(f,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topic=process(input_data=pre_docn, no_list=stop_words,time_table=Time_Table,start=3, limit=20, step=1,file_name=\"doc_neg\",verbose=False)\n",
    "positive_topic=process(input_data=pre_docp, no_list=stop_words,time_table=Time_Table,start=3, limit=20, step=1,file_name=\"doc_pos\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_topic_final=table_modify(positive_topic,topic_pos)\n",
    "neg_topic_final=table_modify(negative_topic,topic_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_topic_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_process(table_topic,input_data):\n",
    "    \n",
    "    \n",
    "    def sentiment_analyzer_scores(sentence):\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        return(\"{}\".format(str(score)))\n",
    "   \n",
    "    sentiment_eval=[sentiment_analyzer_scores(i) for i in input_data]\n",
    "    sent_score=[ast.literal_eval(i) for i in  sentiment_eval]\n",
    "    sent_table=pd.DataFrame(sent_score)\n",
    "    sent_table['Document_No']=sent_table.index\n",
    "    sent_table=sent_table[['compound','Document_No']]\n",
    "    table_sentiment=table_topic.merge(sent_table, on=['Document_No'])\n",
    "    table_sentiment.loc[table_sentiment.Topic_Name==\"None\" ,'compound']=0\n",
    "    return(table_sentiment)\n",
    "    \n",
    "#     return(table2.pivot(index='Document_No',columns='Topic_Name',values='neg'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentiment=table_process(pos_topic_final,pre_docp)\n",
    "neg_sentiment=table_process(neg_topic_final,pre_docn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp=pos_sentiment[['Document_No','Topic_Name','compound']]\n",
    "Tn=neg_sentiment[['Document_No','Topic_Name','compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp1=Tp.pivot(index='Document_No',columns='Topic_Name',values='compound')\n",
    "Tn1=Tn.pivot(index='Document_No',columns='Topic_Name',values='compound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp1=Tp1.drop(['None'],axis=1)\n",
    "Tn1=Tn1.drop(['None'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns=list(set(Tp1.columns).intersection(Tn1.columns))\n",
    "common_columns.append('Document_No')\n",
    "# common_columns.remove('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp1=Tn1.merge(Tp1,on=common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tp1=Tp1.drop(['None'],axis=1)\n",
    "Tp1=Tp1.fillna(0)\n",
    "Tp1=Tp1.merge(neg_topic_final[['Document_No','Review_Date','Reviewer_Score']],on=['Document_No'])\n",
    "# Tp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_col=['general','Decoration','parking','staff_receptionist']\n",
    "Tp1['sum']=Tp1['air_conditioning']+Tp1['bed/bath/shower']+Tp1['service_food']+Tp1['wifi']+Tp1['Room_size']+Tp1['amenities']+Tp1['facility']+Tp1['location']+Tp1['price']\n",
    "Tp1=Tp1[Tp1['sum']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/niloofar/Documents/insight/data/cleaned/hotel1/Final_res\", 'wb') as f:\n",
    "            pickle.dump(Tp1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/niloofar/Documents/insight/data/cleaned/hotel1/Final_res\",'rb') as f:\n",
    "        df=pickle.load(f,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx=Tp1[['Decoration','Review_Date']]\n",
    "min(Tp1.Review_Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=Tp1[(Tp1['Review_Date']>='2017-06-03') & (Tp1['Review_Date']<='2017-07-03')]\n",
    "\n",
    "colnames=['Document_No','sum','Review_Date']\n",
    "df=df.drop(colnames,axis=True)\n",
    "x_column=[i  for i in df.columns if i!=\"Reviewer_Score\"]\n",
    "\n",
    "x_column\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# # # Build a classification task using 3 informative features\n",
    "\n",
    "# # # Build a forest and compute the feature importances\n",
    "\n",
    "# X, y = make_classification(n_samples=df.shape[0],\n",
    "#                            n_features=10,\n",
    "#                            n_informative=3,\n",
    "#                            n_redundant=0,\n",
    "#                            n_repeated=0,\n",
    "#                            n_classes=2,\n",
    "#                            random_state=0,\n",
    "#                            shuffle=False)\n",
    "\n",
    "# forest = ExtraTreesClassifier(n_estimators=250,\n",
    "#                               random_state=0)\n",
    "\n",
    "# forest.fit(X, y)\n",
    "# importances = forest.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "#              axis=0)\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# # Print the feature ranking\n",
    "# print(\"Feature ranking:\")\n",
    "\n",
    "# for f in range(X.shape[1]):\n",
    "#     print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# # Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(X.shape[1]), importances[indices],\n",
    "#        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "# plt.xticks(range(X.shape[1]), indices)\n",
    "# plt.xlim([-1, X.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df.Reviewer_Score\n",
    "X = pd.DataFrame(df, columns = x_column)\n",
    "np.random.seed(seed = 42)\n",
    "# X['random'] = np.random.random(size = len(X))\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from rfpimp import permutation_importances\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "def r2(rf, X_train, y_train):\n",
    "    return r2_score(y_train, rf.predict(X_train))\n",
    "\n",
    "model=permutation_importances(rf, X_train, y_train, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg=RidgeCV(alphas=np.arange(0.05,3.01,0.05),scoring=\"neg_mena_square_error\",cv=None).fit(X_train,y_train)\n",
    "# ridgeReg=Ridge(alpha=reg.alpha)\n",
    "# scores=cross_val_score(ridgeReg,X_train,y_train,cv=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import RidgeCV,Ridge\n",
    "# ridgeReg.fit(X_train,y_train)\n",
    "# index=np.argsort(-ridgeReg.coef_)\n",
    "# ordered=[features[i] for i in index]\n",
    "# dataBar=[go.Bar(x=ordered,y=ridgeReg.coef_[index])]\n",
    "# plotly.offline.iplot(dataBar,filename=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
    "                                                   mode = 'regression',\n",
    "                                                   feature_names = X_train.columns,\n",
    "                                                   categorical_features = [8], \n",
    "                                                   categorical_names = ['CHAS'], \n",
    "                                                   discretize_continuous = True)\n",
    "                                                   \n",
    "np.random.seed(42)\n",
    "exp = explainer.explain_instance(X_valid.values[120], rf.predict, num_features = len(x_column))\n",
    "exp.show_in_notebook(show_all=False) #only the features used in the explanation are displayed\n",
    "\n",
    "exp = explainer.explain_instance(X_valid.values[12], rf.predict, num_features = len(x_column))\n",
    "exp.show_in_notebook(show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone \n",
    "\n",
    "def drop_col_feat_imp(model, X_train, y_train, random_state = 42):\n",
    "    \n",
    "    # clone the model to have the exact same specification as the one initially trained\n",
    "    model_clone = clone(model)\n",
    "    # set random_state for comparability\n",
    "    model_clone.random_state = random_state\n",
    "    # training and scoring the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    # list for storing feature importances\n",
    "    importances = []\n",
    "    \n",
    "    # iterating over all columns and storing feature importance (difference between benchmark and new model)\n",
    "    for col in X_train.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col, axis = 1), y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "    \n",
    "    importances_df = imp_df(X_train.columns, importances)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_col_feat_imp(model, X_train, y_train, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=df.groupby('Review_Date').agg({'bed/bath/shower':['mean']})\n",
    "\n",
    "ss=pd.DataFrame(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predic_fn_rf=lambda x: model.rf.predict_proba(x).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_topic.loc[25,'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_topic[['Dominant_Topic','Keywords']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_topic_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=[sentiment_analyzer_scores(i) for i in pre_docn]\n",
    "aa\n",
    "sent_score=[ast.literal_eval(i) for i in aa]\n",
    "SC=pd.DataFrame(sent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1=neg_topic_final[['Document_No','Topic_Name']]\n",
    "table2=table1.merge(SC, on=['Document_No'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_topic.loc[4784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_topic=process(input_data=pre_docp, no_list=stop_words,time_table=Time_Table,start=2, limit=10, step=1,file_name=\"doc_pos\",verbose=False)\n",
    "\n",
    "with open('doc_pos', 'rb') as f:\n",
    "            dd = pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = corpora.Dictionary(dd)\n",
    "corpus = [words.doc2bow(doc) for doc in dd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=words, corpus=corpus, texts=dd, start=3, limit=15, step=1)\n",
    "opt=coherence_values.index(max(coherence_values))\n",
    "optimal_model=model_list[opt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=15; start=3; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[7]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(optimal_model, corpus, words)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/niloofar/Documents/insight/data/cleaned/hotel2/trip_ad', 'rb') as f:\n",
    "            trip_ad= pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_ad.head()\n",
    "Time_Table=trip_ad[['Date','rating']]\n",
    "file_name='/Users/niloofar/Documents/insight/data/cleaned/hotel2/doc_pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Topic=process(input_data=list(trip_ad['Review']), no_list=stop_words,time_table=Time_Table,start=3, limit=20, step=1,file_name=file_name,verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic[\"Keywords\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic[\"Text\"][2649]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(Topic['Dominant_Topic']).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_table=pd.DataFrame({\"Dominant_Topic\": [8,0,2,4,6],\n",
    "                    \"Topic_Name\":[\"amenities\",\"location\",\"staff\",\n",
    "                                  \"extra_charge\",\"parking\"]})\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic[\"Date\"]= pd.to_datetime(Topic[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=Topic.merge(topic_table, on=['Dominant_Topic'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_topic=table_process(table,list(trip_ad['Review']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_topic[\"Text\"][2653]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_topic[\"compound\"][2653]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "sdate = date(2013, 1, 1)   # start date\n",
    "edate = date(2020, 1, 1)   # end date\n",
    "delta = edate - sdate\n",
    "year=(delta/365).days\n",
    "\n",
    "Date=[]\n",
    "for i in range(0,year):\n",
    "    if i==3:\n",
    "        pt=sdate + timedelta(days=366)\n",
    "    else:    \n",
    "        pt=sdate + timedelta(days=365)\n",
    "    Date.append([sdate,pt])\n",
    "    sdate=pt\n",
    "\n",
    "# Date=[i.strftime('%Y-%m-%d') for i in Date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [date_obj.strftime('%Y%m%d') for date_obj in mondays]\n",
    "\n",
    "# flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "# P=flatten(Date)\n",
    "# P=[i.strftime('%Y-%m') for i in P]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math as m\n",
    "# K=[]\n",
    "# for i in range(0, m.ceil(len(P)/2)):\n",
    "#     a= P[:2]\n",
    "#     K.append(a)\n",
    "#     P=P[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=[Date[i][0].year for i in range(0,len(Date))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_topic_final=sentiment_topic.pivot(index='Document_No',columns='Topic_Name',values='compound').fillna(0)\n",
    "sentiment_topic_final=sentiment_topic_final.merge(sentiment_topic[['Document_No','Date','rating']],on=['Document_No'])\n",
    "sentiment_topic_final.columns=['Document_No','amenities','extra_charge','location','parking','staff','Review_Date','Reviewer_Score']\n",
    "col_list=list(topic_table.Topic_Name)\n",
    "sentiment_topic_final['sum']=sentiment_topic_final[col_list].sum(axis=1)\n",
    "# with open('sentiment_topic_final', 'wb') as f:\n",
    "#     pickle.dump('/Users/niloofar/Documents/insight/data/cleaned/hotel2/sentiment_topic_final', f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sentiment_topic_final.Review_Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=sentiment_topic_final.loc[(sentiment_topic_final.Review_Date>=Date[0][0]) & (sentiment_topic_final.Review_Date<Date[0][1])]\n",
    "# ss.Review_Date = [i.strftime(\"%Y-%m-%d\") for i in ss.Review_Date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=set(ss.Time)\n",
    "nn=sorted([i.strftime(\"%Y-%m-%d\") for i in bb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=pd.DataFrame(ss)\n",
    "ss.Reviewer_Score=[int(i) for i in ss.Reviewer_Score]\n",
    "ss=ss.groupby(['Review_Date'])['Reviewer_Score'].mean()\n",
    "ss=pd.DataFrame(ss)\n",
    "ss=pd.DataFrame({'Time':ss.index,'Score':ss.Reviewer_Score})\n",
    "# ss=pf.DataFrame({'Date':ss.index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
