{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"/Users/niloofar/Downloads/Hotel_Reviews.csv\")\n",
    "# df1=df[df.Hotel_Name==\"Britannia International Hotel Canary Wharf\"]\n",
    "\n",
    "\n",
    "# data_pos = df1['Positive_Review']\n",
    "# data_neg = df1['Negative_Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/niloofar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import English, STOP_WORDS\n",
    "# from en_core_web_lg import *\n",
    "# import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_process(input_data):\n",
    "    \n",
    "    \n",
    "    def sentiment_analyzer_scores(sentence):\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        return(\"{}\".format(str(score)))\n",
    "   \n",
    "    sentiment_eval=[sentiment_analyzer_scores(i) for i in input_data]\n",
    "    sent_score=[ast.literal_eval(i) for i in  sentiment_eval]\n",
    "    sent_table=pd.DataFrame(sent_score)\n",
    "    sent_table['Document_No']=sent_table.index\n",
    "    sent_table=sent_table[['compound','Document_No']]\n",
    "#     table_sentiment=table_topic.merge(sent_table, on=['Document_No'])\n",
    "#     table_sentiment.loc[table_sentiment.Topic_Name==\"None\" ,'compound']=0\n",
    "    return(sent_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp=pos_sent[pos_sent['compound']>0]\n",
    "# fp=pos_sent[pos_sent['compound']<0]\n",
    "# tn=neg_sent[neg_sent['compound']<0]\n",
    "# fn=neg_sent[neg_sent['compound']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall=3483/(3483+1000)\n",
    "# precision=3483/(3483+161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/niloofar/Documents/insight/HotelWatch/data/doc_pos', 'rb') as f:\n",
    "            dd= pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/niloofar/Documents/insight/HotelWatch/data/trip_sentence', 'rb') as f:\n",
    "            df= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/niloofar/Documents/insight/HotelWatch/data/trip_sentence', 'rb') as f:\n",
    "#             df= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df.Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = corpora.Dictionary(dd)\n",
    "corpus = [words.doc2bow(doc) for doc in dd]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "            model =gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=dictionary,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        return model_list, coherence_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=words, corpus=corpus, texts=dd, start=3, limit=7, step=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=coherence_values.index(max(coherence_values))\n",
    "optimal_model=model_list[opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    \n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            \n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=list(df.Review))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.columns=[\"Topic_num\",\"Perc\",\"Keywords\",\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_table=pd.DataFrame({'Topic_num':[0,1,2,3,4,5],\"Topic\":[\"Room_service\",\"view\",\"staff\",\"location\",\"Parking_charges\",\"restaurant\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.merge(final,topic_table, on=\"Topic_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Perc</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2004</td>\n",
       "      <td>room, bathroom, service, thing, corner, deal, ...</td>\n",
       "      <td>No, seriously, don't stay here.</td>\n",
       "      <td>Room_service</td>\n",
       "      <td>-0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>room, bathroom, service, thing, corner, deal, ...</td>\n",
       "      <td>Spend a few more dollars and stay nearby at a ...</td>\n",
       "      <td>Room_service</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2004</td>\n",
       "      <td>room, bathroom, service, thing, corner, deal, ...</td>\n",
       "      <td>Only stayed overnight as we were joining a cru...</td>\n",
       "      <td>Room_service</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2004</td>\n",
       "      <td>room, bathroom, service, thing, corner, deal, ...</td>\n",
       "      <td>We stayed on a Friday night one week &amp; the nex...</td>\n",
       "      <td>Room_service</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2854</td>\n",
       "      <td>room, bathroom, service, thing, corner, deal, ...</td>\n",
       "      <td>Both times were very similar in standards and ...</td>\n",
       "      <td>Room_service</td>\n",
       "      <td>-0.4023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_num    Perc                                           Keywords  \\\n",
       "0        0.0  0.2004  room, bathroom, service, thing, corner, deal, ...   \n",
       "1        0.0  0.2364  room, bathroom, service, thing, corner, deal, ...   \n",
       "2        0.0  0.2004  room, bathroom, service, thing, corner, deal, ...   \n",
       "3        0.0  0.2004  room, bathroom, service, thing, corner, deal, ...   \n",
       "4        0.0  0.2854  room, bathroom, service, thing, corner, deal, ...   \n",
       "\n",
       "                                                Text         Topic  compound  \n",
       "0                  No, seriously, don't stay here.    Room_service   -0.4404  \n",
       "1  Spend a few more dollars and stay nearby at a ...  Room_service    0.4404  \n",
       "2  Only stayed overnight as we were joining a cru...  Room_service    0.0000  \n",
       "3  We stayed on a Friday night one week & the nex...  Room_service    0.0000  \n",
       "4  Both times were very similar in standards and ...  Room_service   -0.4023  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_process(input_data):\n",
    "    \n",
    "    \n",
    "    def sentiment_analyzer_scores(sentence):\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        return(\"{}\".format(str(score)))\n",
    "   \n",
    "    sentiment_eval=[sentiment_analyzer_scores(i) for i in input_data]\n",
    "    sent_score=[ast.literal_eval(i) for i in  sentiment_eval]\n",
    "    sent_table=pd.DataFrame(sent_score)\n",
    "    sent_table['Document_No']=sent_table.index\n",
    "    sent_table=sent_table[['compound','Document_No']]\n",
    "#     table_sentiment=table_topic.merge(sent_table, on=['Document_No'])\n",
    "#     table_sentiment.loc[table_sentiment.Topic_Name==\"None\" ,'compound']=0\n",
    "    return(sent_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=table_process(final.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.concat([final, sent['compound']], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[[\"Text\",\"rating\",\"Date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "finT=pd.concat([final[['compound','Topic',\"Topic_num\"]], df[[\"index\",\"rating\",\"Date\"]]], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1=list(tt['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20476497102382485"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "318/len(tt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46601941747572817"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "192/412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for word in tt1:\n",
    "    if \"restaurant\" in word:\n",
    "        i=i+1\n",
    "print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location, elevator, floor, minute, lobby, area, morning, marketplace, building, way',\n",
       " 'restaurant, desk, port, food, shopping, lot, arena, bar, shop, water',\n",
       " 'room, bathroom, service, thing, corner, deal, towel, shuttle, toilet, rate',\n",
       " 'staff, place, people, breakfast, work, shower, bit, love, distance, luggage',\n",
       " 'time, price, parking, door, experience, beach, car, sleep, friend, wall',\n",
       " 'view, bed, day, downtown, book, street, bay, use, ship, trip'}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(final.Keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dividing comment to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/niloofar/Documents/insight/data/cleaned/hotel2/doc_pos', 'rb') as f:\n",
    "            dd= pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/niloofar/Documents/insight/data/cleaned/hotel2/trip_ad', 'rb') as f:\n",
    "            trip_ad= pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = corpora.Dictionary(dd)\n",
    "corpus = [words.doc2bow(doc) for doc in dd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=words, corpus=corpus, texts=dd, start=3, limit=15, step=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=coherence_values.index(max(coherence_values))\n",
    "optimal_model=model_list[opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=list(trip_ad.Review))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtopic=df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtopic.columns=[\"Dominant_Topic\",\"Perc\",\"Keywords\",\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0    1777\n",
       "0.0     685\n",
       "2.0     184\n",
       "4.0      10\n",
       "6.0       4\n",
       "Name: Dominant_Topic, dtype: int64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(df_topic_sents_keywords.Dominant_Topic).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=dtopic[dtopic['Dominant_Topic']==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2899</td>\n",
       "      <td>park, car, arena, block, towel, space, airline...</td>\n",
       "      <td>Hotel is well located Walking distance to BAYS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2454</td>\n",
       "      <td>park, car, arena, block, towel, space, airline...</td>\n",
       "      <td>This hotel is not only conveniently located bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2817</td>\n",
       "      <td>park, car, arena, block, towel, space, airline...</td>\n",
       "      <td>Last time for this place! Drive 650 miles to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>park, car, arena, block, towel, space, airline...</td>\n",
       "      <td>Silvia Calvino the concierge was exceptionally...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dominant_Topic    Perc  \\\n",
       "10               6.0  0.2899   \n",
       "552              6.0  0.2454   \n",
       "845              6.0  0.2817   \n",
       "1152             6.0  0.2655   \n",
       "\n",
       "                                               Keywords  \\\n",
       "10    park, car, arena, block, towel, space, airline...   \n",
       "552   park, car, arena, block, towel, space, airline...   \n",
       "845   park, car, arena, block, towel, space, airline...   \n",
       "1152  park, car, arena, block, towel, space, airline...   \n",
       "\n",
       "                                                   Text  \n",
       "10    Hotel is well located Walking distance to BAYS...  \n",
       "552   This hotel is not only conveniently located bu...  \n",
       "845   Last time for this place! Drive 650 miles to h...  \n",
       "1152  Silvia Calvino the concierge was exceptionally...  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "tt1=list(tt['Text'])\n",
    "\n",
    "i=0\n",
    "for word in tt1:\n",
    "    if \"park\" in word:\n",
    "        i=i+1\n",
    "print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6630434782608695"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(74+48)/len(tt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# PK=[]\n",
    "# for doc in df.Review:\n",
    "       \n",
    "#         nlp = spacy.load(\"en_core_web_sm\")\n",
    "#         nlp.pipe_names\n",
    "#         tt = nlp(doc)\n",
    "#         ses=[sent.text for sent in tt.sents]\n",
    "#         PK.append(pd.DataFrame({\"index\":i,\"Sent\":ses}))\n",
    "#         i=i+1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = English()\n",
    "# lp= spacy.load(\"en\")\n",
    "# nlp.Defaults.stop_words.update(stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spacy_root(text):\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     doc=[]\n",
    "#     l=[]\n",
    "#     for word in text:\n",
    "#         ss=nlp(word)\n",
    "#         for chunk in ss.noun_chunks:\n",
    "#             l.append(chunk.root.text)\n",
    "#     doc.append(l)\n",
    "#     return(doc)\n",
    "\n",
    "\n",
    "# def lemmatizer(doc):\n",
    "#         # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "#         # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "#         doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "#         doc = u' '.join(doc)\n",
    "#         return nlp.make_doc(doc)\n",
    "    \n",
    "    \n",
    "# def remove_stopwords(doc):\n",
    "#         spacy_nlp = spacy.load('en_core_web_sm')\n",
    "#         spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "#         for i in no_list:  \n",
    "#             STOP_WORDS.add(i)\n",
    "\n",
    "#         for word in STOP_WORDS:\n",
    "#             lexeme = nlp.vocab[word]\n",
    "#             lexeme.is_stop = True\n",
    "\n",
    "#         tokens = [token.text for token in doc if not token.is_stop and token.is_punct != True and  len(token) >=3]\n",
    "#         tokens=[i.lower() for i in tokens]\n",
    "#         return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "\n",
    "\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(input_data)\n",
    "# for token in doc:\n",
    "# #     print([token  for token in doc if token!= '-PRON-'])\n",
    "# #     print(token.text, token.pos_, token.dep_)\n",
    "#       print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.concat(PK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1=topic[[\"Document_No\",\"Dominant_Topic\",\"Topic_Perc_Contrib\",\"Keywords\",\"Date\",\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[\"Document_No\",\"Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=pd.merge(df,topic1,on=[\"Document_No\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[[\"Keywords\",\"Dominant_Topic\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"Document_No\", inplace = True)\n",
    "trip_sentence = df.merge(trip_ad[[\"rating\",\"Date\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/niloofar/Documents/insight/HotelWatch/data/trip_ad\", 'rb') as f:\n",
    "            df= pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/niloofar/Documents/insight/HotelWatch/data/trip_sentence\", 'wb') as f:\n",
    "            pickle.dump(trip_sentence, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "no_list=['hotel','night','nothing','would','could','want','go','recommend','everything','be','was','good','ok','great','poor','miami','YVE',\"friday\",\"saturday\",'monday','week'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/niloofar/Documents/insight/HotelWatch/data/doc_pos\", 'rb') as f:\n",
    "            dd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/Users/niloofar/Documents/insight/HotelWatch/result/Topic\", 'rb') as f:\n",
    "            topic = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1=topic.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ss['Dominant_Topic']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_ad['Review'][422]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(topic1.Keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/niloofar/Documents/insight/data/cleaned/hotel2/sentiment_topic_final', 'rb') as f:\n",
    "            Topic= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/niloofar/Documents/insight/HotelWatch/result/Topic', 'rb') as f:\n",
    "            Topic= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topicf=Topic[[\"Document_No\",\"Dominant_Topic\",\"Keywords\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topicf.set_index(\"Document_No\", inplace = True)\n",
    "df1 = df.merge(Topicf, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic[Topic['Dominant_Topic']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic['Text'][1426]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2=pd.merge(df1[['rating','Date','Dominant_Topic','Keywords']], trip_ad, on=['Date','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2=DF2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic['Keywords'][457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf=Topic[[\"Dominant_Topic\",\"Keywords\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/niloofar/Documents/insight/data/cleaned/hotel2/trip_ad', 'rb') as f:\n",
    "            trip_ad= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic.Keywords[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
